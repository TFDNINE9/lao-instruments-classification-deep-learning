import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import librosa
import librosa.display
from tqdm import tqdm
import json
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# Set style for better looking plots
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

# Create output directory
os.makedirs('visualization_outputs', exist_ok=True)

# Configuration from your script
class Config:
    SAMPLE_RATE = 44100
    SEGMENT_DURATION = 6.0
    N_FFT = 2048
    HOP_LENGTH = 512
    N_MELS = 128
    FMAX = 8000
    DATA_PATH = "dataset"  # Change this to your dataset path
    
    # Data augmentation parameters
    TIME_STRETCH_RANGE = (0.9, 1.1)
    PITCH_SHIFT_RANGE = (-2, 2)
    NOISE_FACTOR = 0.01
    
    # Instrument mapping from your script
    INSTRUMENT_MAPPING = {
        'khean': ['khean', 'khaen', '‡πÅ‡∏Ñ‡∏ô', '‡ªÅ‡∫Ñ‡∫ô'],
        'khong_vong': ['khong', 'kong', '‡∫Ñ‡ªâ‡∫≠‡∫á‡∫ß‡∫ª‡∫á', 'khong_vong'],
        'pin': ['pin', '‡∫û‡∫¥‡∫ô'],
        'ranad': ['ranad', 'nad', '‡∫•‡∫∞‡∫ô‡∫≤‡∫î'],
        'saw': ['saw', 'so', '‡∫ä‡ªç', '‡∫ä‡ªç‡∫≠‡∫π‡ªâ'],
        'sing': ['sing', '‡∫ä‡∫¥‡ªà‡∫á']
    }

def map_instrument_folder(folder_name, class_names):
    """Map a folder name to the corresponding instrument class name"""
    folder_lower = folder_name.lower()
    
    for standard_name, variants in Config.INSTRUMENT_MAPPING.items():
        for variant in variants:
            if variant.lower() in folder_lower:
                return standard_name
    
    # Try to match by name
    for cls in class_names:
        if cls.lower() in folder_lower:
            return cls
    
    return folder_lower

def process_audio_with_best_segment(audio, sr, segment_duration=6.0):
    """Extract the best segment from audio based on energy and spectral content"""
    # Calculate segment length in samples
    segment_len = int(segment_duration * sr)
    
    # If audio is shorter than segment duration, just pad
    if len(audio) <= segment_len:
        return np.pad(audio, (0, segment_len - len(audio)), mode='constant'), [], []
    
    # Create segments with 50% overlap
    hop_len = int(segment_len / 2)
    n_hops = max(1, int((len(audio) - segment_len) / hop_len) + 1)
    segments = []
    segment_starts = []
    
    for i in range(n_hops):
        start = i * hop_len
        end = min(start + segment_len, len(audio))
        if end - start < segment_len * 0.8:  # Skip too short segments
            continue
        segments.append(audio[start:end])
        segment_starts.append(start)
    
    if not segments:  # Just in case no valid segments found
        return audio[:segment_len] if len(audio) >= segment_len else np.pad(audio, (0, segment_len - len(audio)), mode='constant'), [], []
    
    # Calculate metrics for each segment
    metrics = []
    for segment in segments:
        # Energy (RMS)
        rms = np.sqrt(np.mean(segment**2))
        
        # Spectral contrast
        contrast = np.mean(librosa.feature.spectral_contrast(y=segment, sr=sr))
        
        # Spectral flux
        stft = np.abs(librosa.stft(segment))
        if stft.shape[1] > 1:  # Make sure we have at least 2 frames
            flux = np.mean(np.diff(stft, axis=1)**2)
        else:
            flux = 0
        
        score = rms + 0.3 * contrast + 0.2 * flux
        metrics.append(score)
    
    best_idx = np.argmax(metrics)
    return segments[best_idx], metrics, segments

def augment_audio(audio, sr):
    """Apply data augmentation techniques to audio"""
    augmented_samples = []
    augmented_names = []
    
    # Original audio
    augmented_samples.append(audio)
    augmented_names.append("Original")
    
    # Time stretching
    stretch_factor = np.random.uniform(*Config.TIME_STRETCH_RANGE)
    stretched = librosa.effects.time_stretch(audio, rate=stretch_factor)
    # Ensure same length as original
    if len(stretched) > len(audio):
        stretched = stretched[:len(audio)]
    else:
        stretched = np.pad(stretched, (0, max(0, len(audio) - len(stretched))), mode='constant')
    augmented_samples.append(stretched)
    augmented_names.append(f"Time Stretched (rate={stretch_factor:.2f})")
    
    # Pitch shifting
    pitch_shift = np.random.uniform(*Config.PITCH_SHIFT_RANGE)
    shifted = librosa.effects.pitch_shift(audio, sr=sr, n_steps=pitch_shift)
    augmented_samples.append(shifted)
    augmented_names.append(f"Pitch Shifted ({pitch_shift:.2f} steps)")
    
    # Add noise
    noise = np.random.normal(0, Config.NOISE_FACTOR, len(audio))
    noisy = audio + noise
    augmented_samples.append(noisy)
    augmented_names.append(f"Noise Added (factor={Config.NOISE_FACTOR})")
    
    return augmented_samples, augmented_names

def analyze_dataset():
    """Analyze your actual dataset and collect statistics"""
    print("üîç Analyzing your actual dataset...")
    
    # Get all instrument folders
    instrument_folders = [d for d in os.listdir(Config.DATA_PATH) 
                         if os.path.isdir(os.path.join(Config.DATA_PATH, d))]
    
    # Collect class names
    class_names = set()
    for folder in instrument_folders:
        instrument = map_instrument_folder(folder, [])
        class_names.add(instrument)
    class_names = list(class_names)
    
    # Statistics collection
    file_durations = []
    processing_stats = {'success': 0, 'too_short': 0, 'error': 0}
    instrument_data = defaultdict(list)
    instrument_files = defaultdict(list)
    folder_name_variations = defaultdict(list)
    mel_spectrograms = defaultdict(list)
    segment_selection_data = []
    
    # Store an example for each instrument for augmentation visualization
    augmentation_examples = {}
    
    print(f"Found {len(instrument_folders)} folders with {len(class_names)} instrument classes")
    
    # Process each folder
    for folder in tqdm(instrument_folders, desc="Processing folders"):
        instrument = map_instrument_folder(folder, class_names)
        folder_path = os.path.join(Config.DATA_PATH, folder)
        folder_name_variations[instrument].append(folder)
        
        # Get all audio files
        audio_files = [f for f in os.listdir(folder_path) if f.endswith(('.wav', '.mp3', '.m4a', '.flac'))]
        
        for audio_file in tqdm(audio_files, desc=f"Processing {instrument}", leave=False):
            file_path = os.path.join(folder_path, audio_file)
            
            try:
                # Load audio
                audio, sr = librosa.load(file_path, sr=Config.SAMPLE_RATE)
                duration = len(audio) / sr
                file_durations.append(duration)
                
                # Check if too short
                if len(audio) < sr * 0.5:
                    processing_stats['too_short'] += 1
                    continue
                
                # Process with segment selection
                best_segment, segment_scores, all_segments = process_audio_with_best_segment(audio, sr)
                
                # Store segment selection data for visualization
                if len(segment_scores) > 1:  # Only if we have multiple segments
                    segment_selection_data.append({
                        'instrument': instrument,
                        'file': audio_file,
                        'scores': segment_scores,
                        'selected_idx': np.argmax(segment_scores)
                    })
                
                # Extract mel-spectrogram
                mel_spec = librosa.feature.melspectrogram(
                    y=best_segment,
                    sr=sr,
                    n_fft=Config.N_FFT,
                    hop_length=Config.HOP_LENGTH,
                    n_mels=Config.N_MELS,
                    fmax=Config.FMAX
                )
                mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
                mel_spec_normalized = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)
                
                # Store data
                instrument_data[instrument].append({
                    'file_path': file_path,
                    'duration': duration,
                    'audio': best_segment,
                    'mel_spec': mel_spec_normalized
                })
                instrument_files[instrument].append(file_path)
                mel_spectrograms[instrument].append(mel_spec_normalized)
                
                # Store one example per instrument for augmentation visualization
                if instrument not in augmentation_examples:
                    augmentation_examples[instrument] = {
                        'audio': best_segment,
                        'sr': sr,
                        'file': audio_file
                    }
                
                processing_stats['success'] += 1
                
            except Exception as e:
                print(f"Error processing {file_path}: {str(e)}")
                processing_stats['error'] += 1
    
    return {
        'file_durations': file_durations,
        'processing_stats': processing_stats,
        'instrument_data': instrument_data,
        'instrument_files': instrument_files,
        'folder_name_variations': folder_name_variations,
        'class_names': class_names,
        'mel_spectrograms': mel_spectrograms,
        'segment_selection_data': segment_selection_data,
        'augmentation_examples': augmentation_examples
    }

def create_figure_3_1_file_durations(data):
    """Figure 3.1: File duration distribution"""
    durations = data['file_durations']
    
    plt.figure(figsize=(12, 6))
    plt.hist(durations, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Filter Threshold (0.5s)')
    plt.xlabel('File Duration (seconds)', fontsize=12)
    plt.ylabel('Number of Files', fontsize=12)
    plt.title('Distribution of Audio File Durations in Dataset', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Statistics
    short_files = len([d for d in durations if d < 0.5])
    plt.text(0.7, plt.ylim()[1]*0.8, 
             f'Total files: {len(durations)}\nFiles < 0.5s: {short_files}\nMean duration: {np.mean(durations):.2f}s', 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
    
    plt.tight_layout()
    plt.savefig('visualization_outputs/figure_3_1_file_durations.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Figure 3.1: File duration histogram created")

def create_figure_3_2_segment_selection(data):
    """Figure 3.2: Segment selection examples"""
    segment_data = data['segment_selection_data']
    
    if not segment_data:
        print("‚ö† No segment selection data available (all files might be short)")
        return
    
    # Take a few examples from different instruments
    examples = []
    instruments_seen = set()
    for item in segment_data:
        if item['instrument'] not in instruments_seen and len(examples) < 4:
            examples.append(item)
            instruments_seen.add(item['instrument'])
    
    if not examples:
        examples = segment_data[:4]  # Just take first 4 if no variety
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()
    
    for i, example in enumerate(examples[:4]):
        if i >= 4:
            break
            
        scores = example['scores']
        selected_idx = example['selected_idx']
        instrument = example['instrument']
        
        # Bar chart of scores
        bars = axes[i].bar(range(len(scores)), scores, 
                          color=['gold' if j == selected_idx else 'lightblue' 
                                for j in range(len(scores))])
        axes[i].set_title(f'{instrument}: Segment Selection Scores', fontweight='bold')
        axes[i].set_xlabel('Segment Number')
        axes[i].set_ylabel('Combined Score')
        
        # Add score values
        for j, (bar, score) in enumerate(zip(bars, scores)):
            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, 
                        f'{score:.3f}', ha='center', va='bottom', fontsize=8,
                        fontweight='bold' if j == selected_idx else 'normal')
    
    plt.suptitle('Segment Selection Examples', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('visualization_outputs/figure_3_2_segment_selection.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Figure 3.2: Segment selection examples created")

def create_figure_3_3_processing_stats(data):
    """Figure 3.3: Processing statistics"""
    stats = data['processing_stats']
    
    labels = ['Successfully Processed', 'Filtered (Too Short)', 'Processing Errors']
    sizes = [stats['success'], stats['too_short'], stats['error']]
    colors = ['#2ecc71', '#f39c12', '#e74c3c']
    
    plt.figure(figsize=(10, 8))
    wedges, texts, autotexts = plt.pie(sizes, labels=labels, colors=colors,
                                      autopct='%1.1f%%', startangle=90)
    
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(12)
    
    for text in texts:
        text.set_fontsize(11)
        text.set_fontweight('bold')
    
    plt.title('Audio File Processing Statistics', fontsize=16, fontweight='bold', pad=20)
    
    total_files = sum(sizes)
    plt.text(0, -1.3, f'Total Files Processed: {total_files}', ha='center', fontsize=12,
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
    
    plt.tight_layout()
    plt.savefig('visualization_outputs/figure_3_3_processing_stats.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Figure 3.3: Processing statistics created")

def create_figure_3_4_instrument_distribution(data):
    """Figure 3.4: Instrument distribution with naming variations"""
    instrument_files = data['instrument_files']
    folder_variations = data['folder_name_variations']
    
    instruments = list(instrument_files.keys())
    counts = [len(files) for files in instrument_files.values()]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Main distribution
    bars = ax1.bar(instruments, counts, color=sns.color_palette("husl", len(instruments)))
    ax1.set_title('Dataset Distribution by Instrument', fontweight='bold', fontsize=14)
    ax1.set_xlabel('Instrument Type')
    ax1.set_ylabel('Number of Audio Files')
    ax1.tick_params(axis='x', rotation=45)
    
    for bar, count in zip(bars, counts):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                str(count), ha='center', va='bottom', fontweight='bold')
    
    # Folder name variations table
    ax2.axis('tight')
    ax2.axis('off')
    
    table_data = []
    for instrument in instruments:
        variations = folder_variations[instrument]
        table_data.append([instrument, ', '.join(variations)])
    
    table = ax2.table(cellText=table_data,
                     colLabels=['Standard Name', 'Folder Names Found'],
                     cellLoc='left',
                     loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)
    ax2.set_title('Folder Naming Variations Found', fontweight='bold', fontsize=14)
    
    plt.tight_layout()
    plt.savefig('visualization_outputs/figure_3_4_instrument_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Figure 3.4: Instrument distribution created")

def create_figure_3_5_mel_spectrogram_process(data):
    """Figure 3.5: Mel-spectrogram conversion process"""
    # Get a real example from the data
    instrument_data = data['instrument_data']
    
    # Find the first instrument with data
    example_data = None
    for instrument, files_data in instrument_data.items():
        if files_data:
            example_data = files_data[0]
            break
    
    if not example_data:
        print("‚ö† No processed audio data available")
        return
    
    audio = example_data['audio']
    sr = Config.SAMPLE_RATE
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    
    # 1. Original waveform
    t = np.linspace(0, len(audio)/sr, len(audio))
    axes[0,0].plot(t, audio, color='blue', linewidth=0.5)
    axes[0,0].set_title('1. Original Audio Waveform (Best Segment)', fontweight='bold', fontsize=12)
    axes[0,0].set_xlabel('Time (s)')
    axes[0,0].set_ylabel('Amplitude')
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. STFT Spectrogram
    D = librosa.stft(audio, n_fft=Config.N_FFT, hop_length=Config.HOP_LENGTH)
    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
    
    img1 = librosa.display.specshow(S_db, sr=sr, hop_length=Config.HOP_LENGTH,
                                   x_axis='time', y_axis='hz', ax=axes[0,1])
    axes[0,1].set_title('2. STFT Spectrogram', fontweight='bold', fontsize=12)
    axes[0,1].set_ylabel('Frequency (Hz)')
    fig.colorbar(img1, ax=axes[0,1], format='%+2.0f dB')
    
    # 3. Mel-scale Spectrogram
    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr,
                                            n_fft=Config.N_FFT,
                                            hop_length=Config.HOP_LENGTH,
                                            n_mels=Config.N_MELS,
                                            fmax=Config.FMAX)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    
    img2 = librosa.display.specshow(mel_spec_db, sr=sr, hop_length=Config.HOP_LENGTH,
                                   x_axis='time', y_axis='mel', ax=axes[1,0], fmax=Config.FMAX)
    axes[1,0].set_title('3. Mel-scale Spectrogram', fontweight='bold', fontsize=12)
    axes[1,0].set_xlabel('Time (s)')
    axes[1,0].set_ylabel('Mel Frequency')
    fig.colorbar(img2, ax=axes[1,0], format='%+2.0f dB')
    
    # 4. Normalized Mel-spectrogram (the actual one used for training)
    mel_spec_normalized = example_data['mel_spec']
    
    img3 = librosa.display.specshow(mel_spec_normalized, sr=sr, hop_length=Config.HOP_LENGTH,
                                   x_axis='time', y_axis='mel', ax=axes[1,1], fmax=Config.FMAX)
    axes[1,1].set_title('4. Normalized Mel-spectrogram\n(Used for CNN Training)', fontweight='bold', fontsize=12)
    axes[1,1].set_xlabel('Time (s)')
    axes[1,1].set_ylabel('Mel Frequency')
    fig.colorbar(img3, ax=axes[1,1], format='%+2.1f')
    
    plt.tight_layout()
    plt.savefig('visualization_outputs/figure_3_5_mel_spectrogram_process.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Figure 3.5: Mel-spectrogram process created")

def create_figure_3_6_instrument_spectrograms(data):
    """Figure 3.6: Mel-spectrograms from each instrument"""
    mel_spectrograms = data['mel_spectrograms']
    instruments = list(mel_spectrograms.keys())
    
    # Ensure we have 6 instruments in a 2x3 grid
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    for i, instrument in enumerate(instruments):
        if i >= 6:  # Max 6 instruments
            break
            
        if mel_spectrograms[instrument]:
            # Use the first mel-spectrogram for this instrument
            mel_spec = mel_spectrograms[instrument][0]
            
            img = librosa.display.specshow(mel_spec, sr=Config.SAMPLE_RATE, 
                                          hop_length=Config.HOP_LENGTH,
                                          x_axis='time', y_axis='mel',
                                          ax=axes[i], fmax=Config.FMAX)
            axes[i].set_title(f'{instrument}', fontweight='bold', fontsize=14)
            axes[i].set_xlabel('Time (s)')
            axes[i].set_ylabel('Mel Frequency')
            
            # Add colorbar
            cbar = fig.colorbar(img, ax=axes[i], format='%+2.1f')
            cbar.ax.tick_params(labelsize=8)
        else:
            axes[i].text(0.5, 0.5, f'No data for\n{instrument}', ha='center', va='center',
                        transform=axes[i].transAxes, fontsize=12)
            axes[i].set_title(f'{instrument}', fontweight='bold', fontsize=14)
    
    # Hide extra subplots if less than 6 instruments
    for i in range(len(instruments), 6):
        axes[i].axis('off')
    
    plt.suptitle('Mel-spectrograms from Lao Instruments', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('visualization_outputs/figure_3_6_instrument_spectrograms.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Figure 3.6: Instrument mel-spectrograms created")

def create_figure_3_7_augmentation_visualization(data):
    """Figure 3.7: Data augmentation visualization with waveforms and spectrograms"""
    augmentation_examples = data['augmentation_examples']
    
    # Select one instrument for detailed augmentation visualization
    if not augmentation_examples:
        print("‚ö† No augmentation examples available")
        return
    
    # Choose khean (if available) or the first instrument
    instrument = 'khean' if 'khean' in augmentation_examples else list(augmentation_examples.keys())[0]
    
    example = augmentation_examples[instrument]
    audio = example['audio']
    sr = example['sr']
    
    # Apply augmentation
    augmented_samples, augmented_names = augment_audio(audio, sr)
    
    # Create figure with 4 rows (original + 3 augmentations) and 2 columns (waveform + spectrogram)
    fig, axes = plt.subplots(4, 2, figsize=(14, 16))
    
    for i, (sample, name) in enumerate(zip(augmented_samples, augmented_names)):
        # Time domain (waveform)
        t = np.linspace(0, len(sample)/sr, len(sample))
        axes[i, 0].plot(t, sample, linewidth=0.5)
        axes[i, 0].set_title(f'{name} - Waveform', fontweight='bold')
        axes[i, 0].set_xlabel('Time (s)')
        axes[i, 0].set_ylabel('Amplitude')
        axes[i, 0].grid(True, alpha=0.3)
        
        # Frequency domain (mel-spectrogram)
        mel_spec = librosa.feature.melspectrogram(
            y=sample, sr=sr, n_fft=Config.N_FFT, hop_length=Config.HOP_LENGTH,
            n_mels=Config.N_MELS, fmax=Config.FMAX
        )
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        img = librosa.display.specshow(
            mel_spec_db, sr=sr, hop_length=Config.HOP_LENGTH,
            x_axis='time', y_axis='mel', ax=axes[i, 1], fmax=Config.FMAX
        )
        axes[i, 1].set_title(f'{name} - Mel-spectrogram', fontweight='bold')
        axes[i, 1].set_xlabel('Time (s)')
        axes[i, 1].set_ylabel('Mel Frequency')
        
        # Add colorbar to spectrograms
        fig.colorbar(img, ax=axes[i, 1], format='%+2.0f dB')
    
    plt.suptitle(f'Data Augmentation Techniques Applied to {instrument}', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('visualization_outputs/figure_3_7_augmentation_visualization.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Figure 3.7: Data augmentation visualization created using {instrument}")

def create_figure_3_8_augmentation_comparison(data):
    """Figure 3.8: Before vs After data augmentation counts"""
    instrument_files = data['instrument_files']
    
    instruments = list(instrument_files.keys())
    original_counts = [len(files) for files in instrument_files.values()]
    augmented_counts = [count * 4 for count in original_counts]  # 4x augmentation
    
    x = np.arange(len(instruments))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(14, 8))
    bars1 = ax.bar(x - width/2, original_counts, width, label='Original Data',
                   color='lightcoral', alpha=0.8)
    bars2 = ax.bar(x + width/2, augmented_counts, width, label='After Augmentation',
                   color='skyblue', alpha=0.8)
    
    ax.set_xlabel('Instrument Type', fontsize=12)
    ax.set_ylabel('Number of Samples', fontsize=12)
    ax.set_title('Dataset Size: Before vs After Data Augmentation', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(instruments, rotation=45, ha='right')
    ax.legend()
    
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 5,
                   f'{int(height)}', ha='center', va='bottom', fontweight='bold')
    
    total_original = sum(original_counts)
    total_augmented = sum(augmented_counts)
    ax.text(0.02, 0.98, f'Total Original: {total_original}\nTotal Augmented: {total_augmented}\nMultiplier: 4x',
            transform=ax.transAxes, verticalalignment='top',
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
    
    plt.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    plt.savefig('visualization_outputs/figure_3_8_augmentation_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Figure 3.8: Data augmentation comparison created")

def create_figure_3_9_train_test_split(data):
    """Figure 3.9: Train-test split visualization"""
    instrument_files = data['instrument_files']
    
    total_samples = sum(len(files) for files in instrument_files.values()) * 4  # After augmentation
    train_samples = int(total_samples * 0.8)
    test_samples = total_samples - train_samples
    
    sizes = [train_samples, test_samples]
    labels = ['Training Set (80%)', 'Test Set (20%)']
    colors = ['#3498db', '#e74c3c']
    
    fig, ax = plt.subplots(figsize=(10, 8))
    wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                                     startangle=90, pctdistance=0.85)
    
    centre_circle = plt.Circle((0,0), 0.70, fc='white')
    fig.gca().add_artist(centre_circle)
    
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(14)
    
    for text in texts:
        text.set_fontsize(12)
        text.set_fontweight('bold')
    
    ax.text(0, 0.1, f'Total Samples\n{total_samples:,}', ha='center', va='center',
            fontsize=16, fontweight='bold')
    ax.text(0, -0.2, f'Train: {train_samples:,}\nTest: {test_samples:,}', ha='center', va='center',
            fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.7))
    
    ax.set_title('Train-Test Data Split (80-20)', fontsize=16, fontweight='bold', pad=20)
    
    plt.tight_layout()
    plt.savefig('visualization_outputs/figure_3_9_train_test_split.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Figure 3.9: Train-test split visualization created")

def create_figure_3_10_class_weights(data):
    """Figure 3.10: Class weights visualization"""
    instrument_files = data['instrument_files']
    
    instruments = list(instrument_files.keys())
    counts = [len(files) for files in instrument_files.values()]
    
    # Calculate class weights (inverse of frequency)
    total = sum(counts)
    class_weights = [total/(len(instruments) * count) for count in counts]
    
    fig, ax1 = plt.subplots(figsize=(12, 7))
    
    x = np.arange(len(instruments))
    width = 0.4
    
    # Sample counts
    bars1 = ax1.bar(x - width/2, counts, width, color='royalblue', alpha=0.7, label='Sample Count')
    ax1.set_xlabel('Instrument Class', fontsize=12)
    ax1.set_ylabel('Number of Samples', fontsize=12, color='royalblue')
    ax1.tick_params(axis='y', labelcolor='royalblue')
    
    # Add count labels
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{int(height)}', ha='center', va='bottom', color='royalblue', fontweight='bold')
    
    # Class weights on secondary y-axis
    ax2 = ax1.twinx()
    bars2 = ax2.bar(x + width/2, class_weights, width, color='tomato', alpha=0.7, label='Class Weight')
    ax2.set_ylabel('Class Weight', fontsize=12, color='tomato')
    ax2.tick_params(axis='y', labelcolor='tomato')
    
    # Add weight labels
    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{height:.2f}', ha='center', va='bottom', color='tomato', fontweight='bold')
    
    # Title and legend
    plt.title('Class Distribution and Weights', fontsize=14, fontweight='bold')
    
    # Create combined legend
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper center')
    
    ax1.set_xticks(x)
    ax1.set_xticklabels(instruments, rotation=45, ha='right')
    
    # Add explanation
    plt.figtext(0.5, 0.01, 
               'Note: Class weights are inversely proportional to class frequency.\nClasses with fewer samples receive higher weights during training.',
               ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout(rect=[0, 0.03, 1, 0.98])
    plt.savefig('visualization_outputs/figure_3_10_class_weights.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Figure 3.10: Class weights visualization created")

def create_figure_3_11_data_flow(data):
    """Figure 3.11: Data preparation flow diagram"""
    # This is a conceptual diagram that's difficult to create programmatically
    # Instead, create a simple flowchart using matplotlib
    
    instrument_files = data['instrument_files']
    processing_stats = data['processing_stats']
    
    total_files = sum(processing_stats.values())
    successful_files = processing_stats['success']
    augmented_files = successful_files * 4  # 4x augmentation
    train_samples = int(augmented_files * 0.8)
    test_samples = augmented_files - train_samples
    
    stages = ['Raw Audio Files', 'Valid Audio Files', 'Augmented Dataset', 'Training Set', 'Test Set']
    counts = [total_files, successful_files, augmented_files, train_samples, test_samples]
    
    # Create a horizontal bar chart with decreasing width to represent flow
    fig, ax = plt.subplots(figsize=(14, 8))
    
    # Calculate bar widths (decreasing)
    max_width = 0.8
    widths = [max_width * (1 - 0.1 * i) for i in range(len(stages))]
    
    # Define colors for each stage
    colors = ['#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#e74c3c']
    
    for i, (stage, count, width, color) in enumerate(zip(stages, counts, widths, colors)):
        ax.barh(i, count, height=width, color=color, alpha=0.7, edgecolor='black')
        ax.text(count + total_files * 0.01, i, f'{count:,}', va='center', fontweight='bold')
        
    ax.set_yticks(range(len(stages)))
    ax.set_yticklabels(stages, fontsize=12, fontweight='bold')
    ax.set_xlabel('Number of Samples', fontsize=12)
    ax.set_title('Data Preparation Flow', fontsize=16, fontweight='bold')
    
    # Add arrows between bars
    for i in range(len(stages)-1):
        ax.annotate('', xy=(0, i), xytext=(0, i+1),
                   arrowprops=dict(arrowstyle='fancy', color='darkgray', lw=2, 
                                  connectionstyle="arc3,rad=-0.3"))
        
    # Add processing details as text
    details = [
        "Initial dataset collection",
        f"After filtering ({successful_files/total_files*100:.1f}% retained)",
        "After augmentation (4x multiplication)",
        "80% for model training",
        "20% for model testing"
    ]
    
    for i, detail in enumerate(details):
        ax.text(total_files * 0.7, i, detail, va='center', fontsize=10,
               bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.7))
    
    plt.tight_layout()
    plt.savefig('visualization_outputs/figure_3_11_data_flow.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Figure 3.11: Data preparation flow visualization created")

def main():
    """Generate all visualizations based on your dataset"""
    print("üéµ Generating Data Preparation Visualizations...")
    print("=" * 70)
    
    # Check if dataset exists
    if not os.path.exists(Config.DATA_PATH):
        print(f"‚ùå Dataset path '{Config.DATA_PATH}' not found!")
        print("Please update Config.DATA_PATH to point to your dataset directory")
        return
    
    # Analyze your dataset
    data = analyze_dataset()
    
    if not data['instrument_data']:
        print("‚ùå No valid audio data found in the dataset!")
        return
    
    print(f"\nüìä Dataset Analysis Complete:")
    print(f"   ‚Ä¢ Total audio files processed: {len(data['file_durations'])}")
    print(f"   ‚Ä¢ Instruments found: {len(data['class_names'])}")
    print(f"   ‚Ä¢ Processing success rate: {data['processing_stats']['success']}/{sum(data['processing_stats'].values())}")
    
    # Generate visualizations
    print("\nüé® Creating visualizations...")
    
    create_figure_3_1_file_durations(data)
    create_figure_3_2_segment_selection(data)
    create_figure_3_3_processing_stats(data)
    create_figure_3_4_instrument_distribution(data)
    create_figure_3_5_mel_spectrogram_process(data)
    create_figure_3_6_instrument_spectrograms(data)
    create_figure_3_7_augmentation_visualization(data)  # This is the new figure showing augmentation comparison
    create_figure_3_8_augmentation_comparison(data)
    create_figure_3_9_train_test_split(data)
    create_figure_3_10_class_weights(data)
    create_figure_3_11_data_flow(data)
    
    print("=" * 70)
    print("‚úÖ All visualizations have been created successfully!")
    print("üìÅ Check the 'visualization_outputs' folder for all generated figures.")
    
    # Save dataset statistics for reference
    stats_summary = {
        'total_files': len(data['file_durations']),
        'instruments': {inst: len(files) for inst, files in data['instrument_files'].items()},
        'processing_stats': data['processing_stats'],
        'mean_duration': float(np.mean(data['file_durations'])),
        'augmented_total': sum(len(files) for files in data['instrument_files'].values()) * 4
    }
    
    with open('visualization_outputs/dataset_statistics.json', 'w') as f:
        json.dump(stats_summary, f, indent=2)
    
    print("üìä Dataset statistics saved to 'visualization_outputs/dataset_statistics.json'")

if __name__ == "_main_":
    main()